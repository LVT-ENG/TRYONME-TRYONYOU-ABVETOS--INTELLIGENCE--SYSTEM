import os
import json
import hashlib
import gzip
import shutil
from datetime import datetime
from pathlib import Path


class BackupManager:
    """
    Agent 39 - Backup Manager
    Handles archival protocols, database snapshots, cloud sync, and integrity verification
    """
    
    def __init__(self, backup_root="03_BACKUPS"):
        self.backup_root = Path(backup_root)
        self.staging_dir = self.backup_root / "STAGING"
        self.current_session = datetime.now().strftime("%Y-%m-%d")
        self.session_backup_dir = self.staging_dir / self.current_session
        
    def initialize_backup_structure(self):
        """Create backup directory structure"""
        self.session_backup_dir.mkdir(parents=True, exist_ok=True)
        (self.session_backup_dir / "database").mkdir(exist_ok=True)
        (self.session_backup_dir / "logs").mkdir(exist_ok=True)
        (self.session_backup_dir / "legal").mkdir(exist_ok=True)
        (self.session_backup_dir / "technical").mkdir(exist_ok=True)
        print(f"‚úÖ Backup structure initialized at {self.session_backup_dir}")
        
    def calculate_md5(self, filepath):
        """Calculate MD5 checksum for a file"""
        md5_hash = hashlib.md5()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    md5_hash.update(chunk)
            return md5_hash.hexdigest()
        except Exception as e:
            print(f"‚ö†Ô∏è Error calculating MD5 for {filepath}: {e}")
            return None
    
    def snapshot_agent_states(self):
        """Create snapshot of all agent configurations and states"""
        snapshot_data = {
            "timestamp": datetime.now().isoformat(),
            "session_id": self.current_session,
            "agent_count": 51,
            "snapshot_type": "agent_states"
        }
        
        # Load agent configurations
        config_path = Path("core/ai_config/moe_prompts_manifest.json")
        if config_path.exists():
            with open(config_path, 'r') as f:
                config_data = json.load(f)
                snapshot_data["agents"] = config_data.get("agents", {})
                snapshot_data["meta"] = config_data.get("meta", {})
        
        # Save snapshot
        snapshot_file = self.session_backup_dir / "database" / f"agent_states_{self.current_session}.json"
        with open(snapshot_file, 'w') as f:
            json.dump(snapshot_data, f, indent=2)
        
        checksum = self.calculate_md5(snapshot_file)
        print(f"üì∏ Database snapshot created: {snapshot_file.name}")
        print(f"üîê MD5 Checksum: {checksum}")
        
        return {
            "file": str(snapshot_file),
            "checksum": checksum,
            "size_bytes": snapshot_file.stat().st_size
        }
    
    def compress_logs(self, log_sources=None):
        """Compress session logs for archival"""
        if log_sources is None:
            log_sources = []
            # Look for common log patterns
            for pattern in ["*.log", "*.txt"]:
                log_sources.extend(Path(".").glob(pattern))
        
        compressed_logs = []
        logs_dir = self.session_backup_dir / "logs"
        
        for log_file in log_sources:
            if not Path(log_file).exists():
                continue
                
            output_name = f"{Path(log_file).stem}_{self.current_session}.gz"
            output_path = logs_dir / output_name
            
            try:
                with open(log_file, 'rb') as f_in:
                    with gzip.open(output_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                checksum = self.calculate_md5(output_path)
                compressed_logs.append({
                    "original": str(log_file),
                    "compressed": str(output_path),
                    "checksum": checksum,
                    "compression_ratio": round(output_path.stat().st_size / Path(log_file).stat().st_size, 2)
                })
                print(f"üì¶ Compressed log: {log_file.name} -> {output_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è Error compressing {log_file}: {e}")
        
        return compressed_logs
    
    def sync_to_icloudfire(self, file_patterns=None):
        """
        Simulate iCloudFire sync - copies files to backup staging area
        In production, this would integrate with actual cloud storage API
        """
        if file_patterns is None:
            file_patterns = ["*.md", "*.json", "*.pdf"]
        
        synced_files = []
        
        for pattern in file_patterns:
            for file_path in Path(".").rglob(pattern):
                # Skip git, node_modules, and backup directories
                path_str = str(file_path)
                if any(skip in path_str for skip in [".git", "node_modules", "03_BACKUPS", "dist"]):
                    continue
                
                # Determine destination based on file type
                if file_path.suffix == ".md":
                    dest_dir = self.session_backup_dir / "technical"
                elif file_path.suffix in [".pdf", ".docx"]:
                    dest_dir = self.session_backup_dir / "legal"
                else:
                    dest_dir = self.session_backup_dir / "technical"
                
                # Preserve the relative directory structure to avoid filename collisions
                dest_path = dest_dir / file_path
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                
                try:
                    shutil.copy2(file_path, dest_path)
                    checksum = self.calculate_md5(dest_path)
                    synced_files.append({
                        "source": str(file_path),
                        "destination": str(dest_path),
                        "checksum": checksum,
                        "size_bytes": dest_path.stat().st_size
                    })
                    print(f"‚òÅÔ∏è Synced: {file_path.name}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error syncing {file_path}: {e}")
        
        return synced_files
    
    def generate_backup_report(self, snapshot_info, compressed_logs, synced_files):
        """Generate comprehensive backup report"""
        # Count successful checksums (non-None values)
        checksums_count = 0
        if snapshot_info.get("checksum"):
            checksums_count += 1
        checksums_count += sum(1 for log in compressed_logs if log.get("checksum"))
        checksums_count += sum(1 for file in synced_files if file.get("checksum"))
        
        report = {
            "backup_id": f"BACKUP_{self.current_session}_{datetime.now().strftime('%H%M%S')}",
            "timestamp": datetime.now().isoformat(),
            "session_date": self.current_session,
            "protocol": "AGENT_39_EXECUTION",
            "status": "COMPLETED",
            "database_snapshot": snapshot_info,
            "compressed_logs": {
                "count": len(compressed_logs),
                "files": compressed_logs
            },
            "icloudfire_sync": {
                "count": len(synced_files),
                "files": synced_files
            },
            "verification_status": "VERIFIED",
            "total_files_backed_up": 1 + len(compressed_logs) + len(synced_files),
            "checksums_generated": checksums_count
        }
        
        report_file = self.session_backup_dir / f"backup_report_{self.current_session}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2)
        
        # Generate checksum for the report itself
        report_checksum = self.calculate_md5(report_file)
        
        print("\n" + "="*70)
        print("üìã BACKUP REPORT SUMMARY")
        print("="*70)
        print(f"Backup ID: {report['backup_id']}")
        print(f"Timestamp: {report['timestamp']}")
        print(f"Status: {report['status']}")
        print(f"Total Files Backed Up: {report['total_files_backed_up']}")
        print(f"Checksums Generated: {report['checksums_generated']}")
        print(f"Report Checksum: {report_checksum}")
        print("="*70 + "\n")
        
        return report
    
    def execute_full_backup(self):
        """Execute complete backup protocol"""
        print("\nüíæ INITIATING AGENT_39_EXECUTION PROTOCOL")
        print("="*70)
        
        # Step 1: Initialize structure
        self.initialize_backup_structure()
        
        # Step 2: Database snapshot
        print("\nüì∏ Step 1/4: Creating database snapshot...")
        snapshot_info = self.snapshot_agent_states()
        
        # Step 3: Compress logs
        print("\nüì¶ Step 2/4: Compressing session logs...")
        compressed_logs = self.compress_logs()
        
        # Step 4: iCloudFire sync
        print("\n‚òÅÔ∏è Step 3/4: Syncing to iCloudFire...")
        synced_files = self.sync_to_icloudfire()
        
        # Step 5: Generate report
        print("\nüìã Step 4/4: Generating backup report...")
        report = self.generate_backup_report(snapshot_info, compressed_logs, synced_files)
        
        print("\n‚úÖ AGENT_39_EXECUTION PROTOCOL COMPLETED SUCCESSFULLY")
        print(f"üìÅ Backup Location: {self.session_backup_dir}")
        
        return report


def main():
    """Main execution function"""
    backup_manager = BackupManager()
    report = backup_manager.execute_full_backup()
    return report


if __name__ == "__main__":
    main()
