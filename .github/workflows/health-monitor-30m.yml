name: HealthMonitor30Minutos
on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch:
permissions:
  contents: read
  issues: write
jobs:
  health:
    runs-on: ubuntu-latest
    steps:
      - name: Check /api/health
        id: h
        run: |
          # Fetch health data and save response
          curl -s "${{ secrets.HEALTH_URL }}" -o health_response.json
          code=$(curl -s -o /dev/null -w "%{http_code}" "${{ secrets.HEALTH_URL }}")
          echo "code=$code" >> $GITHUB_OUTPUT
          
          # Extract metrics from response
          if [ "$code" = "200" ]; then
            # Parse JSON metrics using jq
            error_rate=$(jq -r '.metrics.error_rate_percent // 0' health_response.json)
            p95_time=$(jq -r '.metrics.p95_response_time_ms // 0' health_response.json)
            response_time=$(jq -r '.metrics.response_time_ms // 0' health_response.json)
            status=$(jq -r '.status // "unknown"' health_response.json)
            active_users=$(jq -r '.metrics.active_users.current // 0' health_response.json)
            
            echo "error_rate=$error_rate" >> $GITHUB_OUTPUT
            echo "p95_time=$p95_time" >> $GITHUB_OUTPUT
            echo "response_time=$response_time" >> $GITHUB_OUTPUT
            echo "status=$status" >> $GITHUB_OUTPUT
            echo "active_users=$active_users" >> $GITHUB_OUTPUT
            
            # Check thresholds
            error_rate_exceeded=$(awk "BEGIN {print ($error_rate > 1.0) ? 1 : 0}")
            p95_exceeded=$(awk "BEGIN {print ($p95_time > 300) ? 1 : 0}")
            
            echo "error_rate_exceeded=$error_rate_exceeded" >> $GITHUB_OUTPUT
            echo "p95_exceeded=$p95_exceeded" >> $GITHUB_OUTPUT
            
            # Print metrics for debugging
            echo "üìä Health Metrics:"
            echo "  Status: $status"
            echo "  Error Rate: $error_rate%"
            echo "  P95 Response Time: $p95_time ms"
            echo "  Current Response Time: $response_time ms"
            echo "  Active Users: $active_users"
            echo "  Error Rate Exceeded: $error_rate_exceeded"
            echo "  P95 Exceeded: $p95_exceeded"
            
            # Fail if any threshold is exceeded or status is not ok
            if [ "$error_rate_exceeded" = "1" ] || [ "$p95_exceeded" = "1" ] || [ "$status" != "ok" ]; then
              echo "‚ùå Health thresholds exceeded or system degraded"
              exit 1
            fi
          else
            echo "‚ùå Health endpoint returned HTTP $code"
            exit 1
          fi

      - name: Alert for threshold violations
        if: failure()
        run: |
          # Determine alert type and message
          ERROR_RATE="${{ steps.h.outputs.error_rate }}"
          P95_TIME="${{ steps.h.outputs.p95_time }}"
          STATUS="${{ steps.h.outputs.status }}"
          CODE="${{ steps.h.outputs.code }}"
          
          # Create detailed alert message
          if [ "$CODE" != "200" ]; then
            ALERT_TYPE="üö® CRITICAL: Health Endpoint Down"
            MSG="$ALERT_TYPE (HTTP $CODE)\n\nThe health endpoint is not responding correctly.\n\nAction Required: Immediate investigation needed.\n\nCommit: ${GITHUB_SHA:0:7}"
            SEVERITY="critical"
          elif [ "${{ steps.h.outputs.error_rate_exceeded }}" = "1" ]; then
            ALERT_TYPE="‚ö†Ô∏è HIGH ERROR RATE ALERT"
            MSG="$ALERT_TYPE\n\nError Rate: $ERROR_RATE% (Threshold: 1.0%)\nP95 Response Time: $P95_TIME ms\nSystem Status: $STATUS\n\nAction Required: Investigate error causes immediately.\n\nCommit: ${GITHUB_SHA:0:7}"
            SEVERITY="high"
          elif [ "${{ steps.h.outputs.p95_exceeded }}" = "1" ]; then
            ALERT_TYPE="‚ö†Ô∏è PERFORMANCE DEGRADATION ALERT"
            MSG="$ALERT_TYPE\n\nP95 Response Time: $P95_TIME ms (Threshold: 300ms)\nError Rate: $ERROR_RATE%\nSystem Status: $STATUS\n\nAction Required: Check system performance and scaling.\n\nCommit: ${GITHUB_SHA:0:7}"
            SEVERITY="medium"
          else
            ALERT_TYPE="‚ö†Ô∏è SYSTEM DEGRADED"
            MSG="$ALERT_TYPE\n\nSystem Status: $STATUS\nError Rate: $ERROR_RATE%\nP95 Response Time: $P95_TIME ms\n\nAction Required: System health check required.\n\nCommit: ${GITHUB_SHA:0:7}"
            SEVERITY="medium"
          fi
          
          # Create GitHub issue
          gh issue create \
            --title "Incident: $ALERT_TYPE" \
            --body "$MSG" \
            --label "ops:incident,severity:$SEVERITY"
          
          # Send Slack notification
          SLACK_MSG="{\"text\":\"$ALERT_TYPE\",\"blocks\":[{\"type\":\"section\",\"text\":{\"type\":\"mrkdwn\",\"text\":\"$MSG\"}}]}"
          curl -X POST \
            -H 'Content-type: application/json' \
            --data "$SLACK_MSG" \
            "${{ secrets.SLACK_WEBHOOK_URL }}"
            
          # Send email alert if configured
          # Email alert logic moved to a separate step with a step-level conditional
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Send email alert
        if: ${{ secrets.EMAIL_WEBHOOK_URL != '' }}
        run: |
          ALERT_TYPE="${{ steps.h.outputs.code != '200' && 'üö® CRITICAL: Health Endpoint Down' || (steps.h.outputs.error_rate_exceeded == '1' && '‚ö†Ô∏è HIGH ERROR RATE ALERT') || (steps.h.outputs.p95_exceeded == '1' && '‚ö†Ô∏è PERFORMANCE DEGRADATION ALERT') || '‚ö†Ô∏è SYSTEM DEGRADED' }}"
          MSG=""
          if [ "${{ steps.h.outputs.code }}" != "200" ]; then
            MSG="$ALERT_TYPE (HTTP ${{ steps.h.outputs.code }})\n\nThe health endpoint is not responding correctly.\n\nAction Required: Immediate investigation needed.\n\nCommit: ${GITHUB_SHA:0:7}"
          elif [ "${{ steps.h.outputs.error_rate_exceeded }}" = "1" ]; then
            MSG="$ALERT_TYPE\n\nError Rate: ${{ steps.h.outputs.error_rate }}% (Threshold: 1.0%)\nP95 Response Time: ${{ steps.h.outputs.p95_time }} ms\nSystem Status: ${{ steps.h.outputs.status }}\n\nAction Required: Investigate error causes immediately.\n\nCommit: ${GITHUB_SHA:0:7}"
          elif [ "${{ steps.h.outputs.p95_exceeded }}" = "1" ]; then
            MSG="$ALERT_TYPE\n\nP95 Response Time: ${{ steps.h.outputs.p95_time }} ms (Threshold: 300ms)\nError Rate: ${{ steps.h.outputs.error_rate }}%\nSystem Status: ${{ steps.h.outputs.status }}\n\nAction Required: Check system performance and scaling.\n\nCommit: ${GITHUB_SHA:0:7}"
          else
            MSG="$ALERT_TYPE\n\nSystem Status: ${{ steps.h.outputs.status }}\nError Rate: ${{ steps.h.outputs.error_rate }}%\nP95 Response Time: ${{ steps.h.outputs.p95_time }} ms\n\nAction Required: System health check required.\n\nCommit: ${GITHUB_SHA:0:7}"
          fi
          EMAIL_DATA="{\"to\":\"${{ secrets.ALERT_EMAIL }}\",\"subject\":\"TRYONU Alert: $ALERT_TYPE\",\"body\":\"$MSG\"}"
          curl -X POST \
            -H 'Content-type: application/json' \
            --data "$EMAIL_DATA" \
            "${{ secrets.EMAIL_WEBHOOK_URL }}"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ALERT_EMAIL: ${{ secrets.ALERT_EMAIL }}
          EMAIL_WEBHOOK_URL: ${{ secrets.EMAIL_WEBHOOK_URL }}

      - name: Success notification
        if: success()
        run: |
          echo "‚úÖ All health checks passed"
          echo "  Error Rate: ${{ steps.h.outputs.error_rate }}%"
          echo "  P95 Response Time: ${{ steps.h.outputs.p95_time }}ms"
          echo "  Active Users: ${{ steps.h.outputs.active_users }}"
